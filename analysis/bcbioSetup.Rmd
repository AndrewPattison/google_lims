---
title: "UMCCR Sample Summary"
author: "Oliver Hofmann"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: readable
    toc: false
    toc_float: false
    code_folding: hide
---

```{r custom, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(scales)
library(gridExtra)
library(gtools)
library(RColorBrewer)
library(knitr)
library(tidyr)
library(rmarkdown)
library(dplyr)
library(DT)
library(readr)
library(forcats)
library(stringr)
library(janitor)
library(googlesheets)
library(here)
library(skimr)
```

## Introduction

Code snippets to generate file summaries (for rsync'ing around) and bcbio sample summaries based on a [Google Spreadsheet](https://docs.google.com/spreadsheets/d/1DwvyfVrgr5TIcYtGVXZeIWWRbld-nuX-4o4z2kZHNWs/edit#gid=0), aka the dreadful LIMS stand-in.


## Workflow

The process is currently manual, i.e., run code block by code block in RStudio as we are still trying to sort out edge cases. It is also useful being able to inspect the data frames listing files, phenotypes and batches before kicking off a bcbio run. A typical workflow includes the steps below. 

### 1. Setting up project information

This currently requires specifying both project name and the type of sequencing data (`WGS`, `WTS`, `10X-WGS`, etc.) and will pull out all files of that type and project which do not have results associated with them yet. This should be generalized at some point to support sample extraction by Illumina RunID or by patient ID. The exact names can be copied from the `Project` and `Type` columns of the [Google-LIMS sheet](https://docs.google.com/spreadsheets/d/1DwvyfVrgr5TIcYtGVXZeIWWRbld-nuX-4o4z2kZHNWs/edit?usp=sharing).

```{r project, eval=FALSE}
PROJECT <- 'Avner-KRAS-wt'
TYPE <- 'WTS'
```

### 2. Import data from Google spreadsheet

This step generates a data frame from the Google spreadsheet, unifying variable names and simplifying subject IDs for bcbio along the way. It replacesg empty `results` cells (which indicate samples that still need processing) with a `-` to distinguish from true NAs and gets rid of whitespace in subject identifiers. Otherwise standard data cleanup with `janitor`. We are also creating a timestamped backup of the Google Doc each time it is accessed, just in case.

```{r importData, message=FALSE, eval=FALSE}
# Register UMCCR spreadsheet. Use cached authentication
gs_auth(token="./googlesheets_token.rds")
samples_gs <- gs_key('1DwvyfVrgr5TIcYtGVXZeIWWRbld-nuX-4o4z2kZHNWs')

# Create a backup copy each time we access the sample info
tm <- as.POSIXlt(Sys.time(), "UTC", "%Y-%m-%dT%H:%M:%S")
timestamp <- strftime(tm , "%Y-%m-%dT%H%M")
filename <- paste(timestamp, 'backup.csv', sep='_')

samples_gs %>%
  gs_download(ws='Sheet1', to=here::here('output', 'backup', filename)) 

# Tweak for analysis
samples <- samples_gs %>%
  gs_read(ws='Sheet1') %>%
  clean_names() %>%
  remove_empty(c('rows', 'cols')) %>%
  mutate(subject_id = gsub(' ', '.', subject_id)) %>%
  mutate(results = ifelse(is.na(results), '-', results))
```


### 3. Create rsync list: WGS tumor/normal use case

Find all FASTQs for samples that still need to be processed. The generated output file can be used on Spartan to generate (hard)links ready for `rsync` to NCI or `aws s3 cp` to, well, S3. It also generates 'better' sample names by renaming the FASTQ files from the filename to the sample name.

```{r csv, eval=FALSE}
# Keep rows matching project and type requested; extract path to FASTQ
# and generate new file name for these
bcbio <- samples %>%
  filter(project == PROJECT & 
           type == TYPE &
           results == '-') %>%
  select(fastq, sample_id, sample_name, subject_id, phenotype) %>%
  mutate(runname = basename(fastq)) %>%
  mutate(targetname = paste(runname, sample_name, sep='_')) 

# Former filters currently not in use, just for reference
#   results != 'Failed' &
#   !str_detect(results, 'cephfs')) %>%

# Create a file list ready for linking
read1 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R1_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R1_001.fastq.gz', sep='')) %>%
  select(from, to)

read2 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R2_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R2_001.fastq.gz', sep='')) %>%
  select(from, to)

link <- rbind(read1, read2)

# Write the file list for linking on Spartan
write.table(paste('ln', link$from, link$to, sep=' '),
            file=here::here('output', paste(timestamp, '_', PROJECT, '_', TYPE, '_files.txt', sep='')),
            col.names=FALSE,
            row.names=FALSE,
            quote=FALSE)
```

### 4. Create bcbio config: WGS tumor/normal use case

Generate a `.csv` sample descriptor, assigning patient/family identifiers for Peddy, assigning batches based on the subject id, and pairing tumor/normals as needed. Samples with the exact same name (but from different runs) are expected to be top-ups and will subsequently be merged using `bcbio_prepare_samples.py`.

```{r bcbioconfig, eval=FALSE}
# Use the same information to generate the bcbio CSV file
# Keep the required columns and add information for peddy
template <- bcbio %>% 
  mutate(family_id=subject_id) %>%
  mutate(individual_id=subject_id) %>%
  select(samplename=targetname, 
         description=sample_name,
         batch=subject_id,
         phenotype, family_id, individual_id)

# Handle cases where a normal matches more than tumor sample. 
# Courtesy of [Peter Diakumis](https://github.com/pdiakumis)
get_new_batch <- function(batch, phenotype) {
  stopifnot(length(batch) == length(phenotype))
  n <- length(batch)

  new_batch <- vector(mode = "character", length = n)
  x <- table(phenotype)

  if (n > 2) {
    tum_ns <- seq_len(x["tumor"])
    new_batch[phenotype == "tumor"] <- paste0(batch[phenotype == "tumor"], '_', tum_ns)
    # assume only one normal for now
    new_batch[phenotype == "normal"] <- paste0(batch[phenotype == "tumor"], '_', tum_ns, collapse = ";")
  } else {
    new_batch <- batch
  }
  return(new_batch)
}

# Remove top-up samples (samples with the exact same description)
# before calculating batch assignments. We'll add them back in later
unique_samples <- template %>%
  group_by(description) %>%
  filter(row_number(description) == 1)

# Create batch information
batches <- unique_samples %>%
  group_by(batch) %>%
  mutate(new_batch = get_new_batch(batch, phenotype)) 

# Bring back the batch information to the main template
template <- template %>%
  left_join(batches, by=c('description')) %>%
  select(samplename=samplename.x,
         description,
         batch=new_batch,
         phenotype=phenotype.x,
         family_id=family_id.x,
         individual_id=individual_id.x)

# Since we are merging add the R1/R2 readpair information. Could probably 
# be more elegant...
template_R1 <- template %>%
  mutate(samplename = paste(samplename, '_R1_001.fastq.gz', sep=''))
template_R2 <- template %>%
  mutate(samplename = paste(samplename, '_R2_001.fastq.gz', sep=''))
template <- rbind(template_R1, template_R2)

write.table(template,
            file=here::here('output', paste(timestamp, '_', PROJECT, '_', TYPE, '.csv', sep='')),
            row.names=FALSE,
            col.names=TRUE,
            quote=FALSE,
            sep=',')
```

### 5. Data set generation

Assuming a run on Spartan:

* copy over the two newly generated files to Spartan (`scp TIMESTAMP_PROJECT* spartan:~/`)
* create a new project directory on Spartan (`mkdir /data/cephfs/punim0010/projects/myproject; cd /data/cephfs/punim0010/projects/myproject`
* create a data directory and link required files (`mkdir data; cd data; mv ~/TIMESTAMP_PROJECT* .; sh TIMESTAMP_PROJECT_files.txt`)

This should result in all required FASTQ files being present in the current data directories. Errors usually mean permissions need to be updated, or the sample name is incorrectly captured in the spreadsheet. The next step merges FASTQ files where necessary and generates the final sample manifest for bcbio:

* Copy over the merge script (`cp /data/cephfs/punim0010/projects/std_workflow/merge.sh .`) and edit the `--csv` flag to use the generated sample sheet (`TIMESTAMP_PROJECT_files.csv`)
* Submit the job to the scheduler (`sbatch merge`)

This will result in a new `merged` CSV alongside our standard template:

* `mv *-merged.csv ..; cd ..`
* `bcbio_nextgen.py -w template ../std_workflow/std_workflow_cancer.yaml TIMESTAMP_PROJECT-merged.csv data/merged/*.gz`
* `cp ../std_workflows/run.sh TIMESTAMP_PROJECT-merged/work; cd TIMESTAMP_PROJECT-merged/work`

Edit the `run.sh` job submission script to point at the newly generated bcbio YAML template and you should be good to go.


## Alternative workflows

### WGS Positive Controls

Positive controls use a slightly different template (and only require germline calls); we also do not merge files prior to processing. We do add a timestamp to the batch based on the Illumina Run Identifier to be abe to sort / filter by time of the run in MultiQC and define the gold standard / benchmarkfiles to use for `rtg eval`:

```{r posControlFiles, eval=FALSE}
# Define what validation data to use; all of these are
# shipped with bcbio so we don't have to prep standards
validations <- c('giab-NA12878', 'giab-NA24385', 'giab-NA24631')
names(validations) <- c('NA12878', 'NA24385', 'NA24631')

# rsync command as before
bcbio <- samples %>%
  filter(project == 'Positive-control' & 
           type == 'WGS' &
           results == '-') %>%
  mutate(targetname = paste(illumina_id, sample_name, sep='_')) %>%
  select(fastq, sample_id, sample_name, targetname, subject_id, phenotype)

# Create a file list ready for linking
read1 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R1_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R1_001.fastq.gz', sep='')) %>%
  select(from, to)

read2 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R2_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R2_001.fastq.gz', sep='')) %>%
  select(from, to)

link <- rbind(read1, read2)

# Write the file list for linking
write.table(paste('ln', link$from, link$to, sep=' '),
            file=here::here('output', paste(timestamp, '_posControl_files.txt', sep='')),
            col.names=FALSE,
            row.names=FALSE,
            quote=FALSE)

# As before, generate bcbio CSV. Simpler process here.
template <- bcbio %>% 
  mutate(family_id=subject_id) %>%
  mutate(individual_id=subject_id) %>%
  select(samplename=targetname, 
         batch=subject_id,
         phenotype, family_id, individual_id)

# Add timestamp to description, batch
template <- template %>%
  separate(samplename, c('timestamp', 'machine', 'run'),
           sep='_', extra="drop", 
           remove=FALSE) %>%
  mutate(description=paste(timestamp, machine, run, batch, sep='-')) %>%
  mutate(batch=paste('batch', timestamp, machine, run, batch, sep='-')) %>%
    select(samplename, description, batch, phenotype,
         family_id, individual_id)

# Add the validation information
template <- template %>% 
  mutate(validate=paste(validations[individual_id], 
                        'truth_small_variants.vcf.gz', sep='/')) %>%
  mutate(validate_regions=paste(validations[individual_id], 
                                'truth_regions.bed', sep='/')) %>%
  mutate(validate_batch=individual_id)
  
write.table(template,
            file=here::here('output', paste(timestamp, '_posControl.csv', sep='')),
            row.names=FALSE,
            col.names=TRUE,
            quote=FALSE,
            sep=',')
```

### WGS Negative Controls

For negative controls we only need to collect all relevant files in one place (hardlinks again) and check the results with `find . -type f -size +500M -exec ls -sh {} \; 2> /dev/null`.

```{r negControl, eval=FALSE}
# rsync command as before
bcbio <- samples %>%
  filter(project == 'Negative-control') %>%
  mutate(targetname = paste(illumina_id, sample_name, sep='_')) %>%
  select(fastq, sample_id, sample_name, targetname, subject_id, phenotype)

# Create a file list ready for linking
read1 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R1_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R1_001.fastq.gz', sep='')) %>%
  select(from, to)

read2 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R2_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R2_001.fastq.gz', sep='')) %>%
  select(from, to)

link <- rbind(read1, read2)

# Write the file list for linking
write.table(paste('ln', link$from, link$to, sep=' '),
            file=here::here('output', paste(timestamp, '_negControl_files.txt', sep='')),
            col.names=FALSE,
            row.names=FALSE,
            quote=FALSE)
```

### WTS

For WTS samples the process is easier since we do not need to handle phenotypes or batch pairing:

```{r rnasync, eval=FALSE}
# Use the same information to generate the bcbio CSV file
# Keep the required columns and add information for peddy
template <- bcbio %>% 
  mutate(family_id=subject_id) %>%
  mutate(individual_id=subject_id) %>%
  select(samplename=targetname, 
         description=sample_name,
         batch=subject_id,
         phenotype, family_id, individual_id)

# Since we are merging add the R1/R2 readpair information. Could probably 
# be more elegant...
template_R1 <- template %>%
  mutate(samplename = paste(samplename, '_R1_001.fastq.gz', sep=''))
template_R2 <- template %>%
  mutate(samplename = paste(samplename, '_R2_001.fastq.gz', sep=''))
template <- rbind(template_R1, template_R2)

write.table(template,
            file=here::here('output', paste(timestamp, '_', 
                                            PROJECT, '_', 
                                            TYPE, '.csv', sep='')),
            row.names=FALSE,
            col.names=TRUE,
            quote=FALSE,
            sep=',')
```


### Create bcbio config (positive WTS controls)

Different configuration for WTS controls -- we do not validate against a benchmark / standard. 

```{r posControlFilesWTS, eval=FALSE}
# rsync command as before
bcbio <- samples %>%
  filter(project == 'Positive-control' & 
           type == 'WTS' &
           results == '-') %>%
  mutate(targetname = paste(illumina_id, sample_name, sep='_')) %>%
  select(fastq, sample_id, sample_name, targetname, subject_id, phenotype)

# Create a file list ready for linking
read1 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R1_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R1_001.fastq.gz', sep='')) %>%
  select(from, to)

read2 <- bcbio %>%
  mutate(from = paste(fastq, '/', sample_id, '_R2_001.fastq.gz', sep='')) %>%
  mutate(to = paste(targetname, '_R2_001.fastq.gz', sep='')) %>%
  select(from, to)

link <- rbind(read1, read2)

# Write the file list for linking
write.table(paste('ln', link$from, link$to, sep=' '),
            file=here::here('output', 'posControl_WTS_files.txt'),
            col.names=FALSE,
            row.names=FALSE,
            quote=FALSE)

# As before, generate bcbio CSV. Simpler process here.
template <- bcbio %>% 
  mutate(family_id=subject_id) %>%
  mutate(individual_id=subject_id) %>%
  select(samplename=targetname, 
         batch=subject_id,
         phenotype, family_id, individual_id)

# Add timestamp to description, batch
template <- template %>%
  separate(samplename, c('timestamp', 'machine', 'run'),
           sep='_', extra="drop", 
           remove=FALSE) %>%
  mutate(description=paste(timestamp, machine, run, batch, sep='-')) %>%
  mutate(batch=paste('batch', timestamp, machine, run, batch, sep='-')) %>%
    select(samplename, description, batch, phenotype,
         family_id, individual_id)

# Since we are merging add the R1/R2 readpair information. Could probably 
# be more elegant...
template_R1 <- template %>%
  mutate(samplename = paste(samplename, '_R1_001.fastq.gz', sep=''))
template_R2 <- template %>%
  mutate(samplename = paste(samplename, '_R2_001.fastq.gz', sep=''))
template <- rbind(template_R1, template_R2)

write.table(template,
           file=here::here('output', 'posControl_WTS.csv'),
           row.names=FALSE,
           col.names=TRUE,
           quote=FALSE,
           sep=',')
```


